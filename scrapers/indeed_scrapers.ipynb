{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "054fe0d4",
   "metadata": {},
   "source": [
    "#### Indeed Scraper Version 1 (only gets partial summary)\n",
    "Almost entirely taken from https://github.com/vittoriotriassi/jobs_scraper with added search option for level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adff3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entry_level, mid_level, senior_level\n",
    "#https://www.indeed.com/jobs?q=-&l=nyc&explvl=mid_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed76986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class JobsScraper:\n",
    "    \"\"\"JobsScraper is a simple job postings scraper for Indeed.\"\"\"\n",
    "\n",
    "    def __init__(self, country: str, position: str, location: str, level: str, pages: int, max_delay: int = 0, full_urls: bool = False):\n",
    "        \"\"\"\n",
    "        Create a JobsScraper object.\n",
    "        Parameters\n",
    "        ------------\n",
    "        country: str\n",
    "            Prefix country.\n",
    "            Available countries:\n",
    "            AE, AQ, AR, AT, AU, BE, BH, BR, CA, CH, CL, CO,\n",
    "            CZ, DE, DK, ES, FI, FR, GB, GR, HK, HU, ID, IE,\n",
    "            IL, IN, IT, KW, LU, MX, MY, NL, NO, NZ, OM, PE,\n",
    "            PH, PK, PL, PT, QA, RO, RU, SA, SE, SG, TR, TW,\n",
    "            US, VE, ZA.\n",
    "        position: str\n",
    "            Job position.\n",
    "        location: str\n",
    "            Job location.\n",
    "        pages: int\n",
    "            Number of pages to be scraped. Each page contains 15 results.\n",
    "        max_delay: int, default = 0\n",
    "            Max number of seconds of delay for the scraping of a single posting.\n",
    "        full_urls: bool, default = False\n",
    "            If set to True, it shows the job url column not truncated in the DataFrame.\n",
    "        \"\"\"\n",
    "        if country.upper() == \"US\":\n",
    "            self._url = 'https://indeed.com/jobs?q={}&l={}&explvl={}'.format(position, location, level)\n",
    "        else:\n",
    "            self._url = 'https://{}.indeed.com/jobs?q={}&l={}&explvl={}'.format(country, position, location, level)\n",
    "        self._country = country\n",
    "        self._headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36'}\n",
    "        self._pages = pages\n",
    "        self._max_delay = max_delay\n",
    "        self._jobs = []\n",
    "        print(self._url)\n",
    "        if full_urls:\n",
    "            pd.set_option('display.max_colwidth', None)\n",
    "        else:\n",
    "            pd.reset_option('display.max_colwidth')\n",
    "\n",
    "\n",
    "    def _extract_page(self, page):\n",
    "\n",
    "        with requests.Session() as request:\n",
    "            r = request.get(url=\"{}&start={}\".format(self._url, page), headers=self._headers)\n",
    "\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "        return soup\n",
    "\n",
    "\n",
    "    def _transform_page(self, soup):\n",
    "\n",
    "        jobs = soup.find_all('div', class_='job_seen_beacon')\n",
    "\n",
    "        for job in jobs:\n",
    "\n",
    "            try:\n",
    "                title = job.find(\n",
    "                    'h2', class_='jobTitle').text.strip().replace('\\n', '')\n",
    "            except:\n",
    "                title = None\n",
    "            try:\n",
    "                company = job.find(\n",
    "                    'span', class_='companyName').text.strip().replace('\\n', '')\n",
    "            except:\n",
    "                company = None\n",
    "            try:\n",
    "                summary = job.find(\n",
    "                    'div', {'class': 'job-snippet'}).text.strip().replace('\\n', '')\n",
    "            except:\n",
    "                summary = None\n",
    "\n",
    "            if job.find('div', class_='companyLocation'):\n",
    "                try:\n",
    "                    location = job.find(\n",
    "                        'div', class_='companyLocation').text.strip().replace('\\n', '')\n",
    "                except:\n",
    "                    location = None\n",
    "            else:\n",
    "                try:\n",
    "                    location = job.find(\n",
    "                        'span', class_='location').text.strip().replace('\\n', '')\n",
    "                except:\n",
    "                    location = None\n",
    "            try:\n",
    "                href = job.parent.a.get('href')\n",
    "                if self._country.upper() == \"US\":\n",
    "                    job_url = 'https://indeed.com{}'.format(href)\n",
    "                else:\n",
    "                    job_url = 'https://{}.indeed.com{}'.format(self._country, href)\n",
    "            except:\n",
    "                job_url = None\n",
    "            try:\n",
    "                salary = job.find(\n",
    "                    'span', class_='salary-snippet').text.strip().replace('\\n', '')\n",
    "            except:\n",
    "                salary = None\n",
    "\n",
    "            job = {\n",
    "                'title': title,\n",
    "                'location': location,\n",
    "                'company': company,\n",
    "                'summary': summary,\n",
    "                'salary': salary,\n",
    "                'url': job_url\n",
    "            }\n",
    "\n",
    "            self._jobs.append(job)\n",
    "\n",
    "            print(\"Scraping {}...\".format(title))\n",
    "\n",
    "            if self._max_delay > 0:\n",
    "                sleep(random.randint(0, self._max_delay))\n",
    "\n",
    "\n",
    "    def scrape(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform the scraping for the parameters provided in the class constructor.\n",
    "        If duplicates are found, they get dropped.\n",
    "        Returns\n",
    "        ------------\n",
    "        df: pd.DataFrame\n",
    "            Return a scraped Dataframe.\n",
    "        \"\"\"\n",
    "\n",
    "        for i in tqdm(range(0, self._pages * 10, 10), desc = \"Scraping in progress...\", total = self._pages):\n",
    "\n",
    "            page = self._extract_page(i)\n",
    "            self._transform_page(page)\n",
    "\n",
    "        df = pd.DataFrame(self._jobs)\n",
    "        df.drop_duplicates(inplace=True)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa42e970",
   "metadata": {},
   "source": [
    "Download job data for each categorie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a17a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "my_list = [[\"internship\", \"\"], [\"a\", \"entry_level\"], [\"a\", \"mid_level\"], [\"a\", \"senior_level\"]]\n",
    "my_dfs = []\n",
    "for e in my_list:\n",
    "    scraper = JobsScraper(country=\"US\", \n",
    "                          position=e[0], \n",
    "                          location=\"\",\n",
    "                          level=e[1],\n",
    "                          pages=200, \n",
    "                          full_urls=True, \n",
    "                          max_delay=3)\n",
    "    df = scraper.scrape()\n",
    "    if e[0] == \"internship\":\n",
    "        print(\"YESSSSS\")\n",
    "        df[\"level\"] = \"internship\"\n",
    "    else:\n",
    "        df[\"level\"] = e[1]\n",
    "    my_dfs.append(df)\n",
    "\n",
    "df_all = pd.concat(my_dfs)\n",
    "\n",
    "csv_name = \"df_all_len_\" + str(len(df_all)) + \".csv\"\n",
    "\n",
    "df_all.to_csv(csv_name) \n",
    "print(csv_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb635d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download one by one\n",
    "#%%time\n",
    "scraper = JobsScraper(country=\"US\", \n",
    "                      position=\"a\", \n",
    "                      location=\"\",\n",
    "                      level=\"senior_level\",\n",
    "                      pages=1, \n",
    "                      full_urls=True, \n",
    "                      max_delay=3)\n",
    "df = scraper.scrape()\n",
    "len(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c15739",
   "metadata": {},
   "source": [
    "#### Indeed Scraper Version 2 with VPN changing (gets full summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4ea560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from random import randint\n",
    "from time import sleep\n",
    "import random\n",
    "import time \n",
    "from nordvpn_switcher import initialize_VPN, rotate_VPN, terminate_VPN\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0\"\n",
    "}\n",
    "\n",
    "#my_list = [ [\"the\", \"senior_level\"], [\"the\", \"entry_level\"], [\"intern\", \"\"], [\"the\", \"mid_level\"] ]\n",
    "my_list = [ [\"intern\", \"\"] ]\n",
    "\n",
    "for e in my_list:\n",
    "    \n",
    "    len_jobs_before = 0\n",
    "    count = 0\n",
    "    job = {}\n",
    "    jobs = []\n",
    "    job_name = e[0]\n",
    "    level= e[1]\n",
    "    page_start = 0\n",
    "    page_end = 25\n",
    "\n",
    "    for i in range((page_start * 10), (page_end * 10), 10):\n",
    "\n",
    "        try:\n",
    "            url = \"https://www.indeed.com/jobs?q=\" + job_name + \"&l=US&explvl=\" + level + \"&start=\" + str(i)\n",
    "            print(url)\n",
    "            api_url = \"https://www.indeed.com/viewjob?viewtype=embedded&jk={job_id}\"\n",
    "\n",
    "            soup = BeautifulSoup(requests.get(url, headers=headers).content, \"html.parser\")\n",
    "\n",
    "            for job in soup.select('a[id^=\"job_\"]'):\n",
    "                job_id = job[\"id\"].split(\"_\")[-1]\n",
    "                s = BeautifulSoup(\n",
    "                    requests.get(api_url.format(job_id=job_id), headers=headers).content,\n",
    "                    \"html.parser\",\n",
    "                )\n",
    "                title = s.title.get_text(strip=True).split(\" - \", 1)[0]\n",
    "                description = s.select_one(\"#jobDescriptionText\").get_text(strip=True, separator=\"\\n\")\n",
    "                job = {\n",
    "                    'title': title,\n",
    "                    'description': description,\n",
    "                    'level': level\n",
    "                    }\n",
    "                jobs.append(job)\n",
    "                \n",
    "            print(\"Page\", int(i/10), \"done.\")\n",
    "            print(\"Len Dict\", len(jobs))\n",
    "            sleep(random.uniform(0.7, 2.0))\n",
    "            \n",
    "            if len_jobs_before == len(jobs): #rotate vpn if no additional jobs have been downloaded\n",
    "                count = count + 1\n",
    "                print(\"Break Count:\", count)\n",
    "                print(\"Changing VPN.\")\n",
    "                initialize_VPN(save=1, area_input=['complete rotation'])\n",
    "                rotate_VPN()\n",
    "                print(\"VPN changed.\")\n",
    "                sleep(5)\n",
    "            \n",
    "            if count >= 10: #stop trying if no new jobs have been download 10 times in a row\n",
    "                break\n",
    "\n",
    "            len_jobs_before = len(jobs)\n",
    "        except: print(\"Error.\")\n",
    "            \n",
    "\n",
    "    df = pd.DataFrame(jobs)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    csv_name = \"df_full_sum\" + \"_job_\" + job_name + \"_lvl_\" + level + \"_len_\" + str(len(df))+ \".csv\"\n",
    "    print(csv_name)\n",
    "    df.to_csv(csv_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
